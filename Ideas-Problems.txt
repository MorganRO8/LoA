Run workflow on the single “big dataset” paper

LoA Ideas:

json generator

expected type of answer

menu option for data management

organize user inputs at top for interactive mode

Set up some kind of requeue system or server to handle the different types of resources needed by different parts of the program

add more preprint servers to scrape
-researchgate
https://oa.mg/data

statistical threshold based repetition system

big llm based excel doc editing

Most important:

make sure it can recognize when something is not in the text https://chat.openai.com/c/921a2279-77de-436e-86e1-b971c3c1846c

SI scraping (chemrxiv 3d coords)

Make version of test that can be used to measure timings on each function of the code, maybe even add an in-program calculator to estimate job time, with a 95% accuracy range in addition to the mean.

add in table extraction! (well, better table extraction) https://chat.openai.com/c/3fd839c8-660c-4b02-9539-7f1ad9f72806

Get a gd database already, jesus.


Things we want to recognize:
structure or SMILES string, formal charge if possible, FQY, abs/emi wavelengths



Okay, gotta rebuild snorkel_train, here's my current gameplan:

1) Data Collection: I have 3 fully functional paper-scraping mechanisms set up and working, will add more in the future, but I want to get the rest of this working first.

2) Pre-processing: Using unstructured I am parsing all the scraped docs into plaintext. There is a function of this that will recognize tables and convert them to csv format, which should be routed to another folder for tables so they can be processed differently. For document types such as XML, a simpler parsing method is used, as we don't need ocr or anything like that.

3) Labeling for streamlining extraction: Use Snorkel and labeling functions to label spans of text in the data. We should prolly use a multi-step process here:

a. Automated Labeling: Use a set of labeling functions generated by user examples to identify spans of text that are likely to contain an answer on a subset of our total data. Make separate labelling functions per user-specified task (ex: extracting chemical name, extracting quantum yield, extracting solvent, etc.)

b. Train a labelmodel for each task using the generated labelling heuristics and generate a label dataset with it, also for each task. 

c. Train another model for each task (let's call them the gatekeepers) to predict based only on the text itself if a span of text contains an answer or not using the label dataset.

d. Start running the gatekeepers on paragraphs from papers one by one, add any that seem not to contain an answer to a new training dataset with the span of text and an empty string as the answer. This will come in handy later. Any spans of text that do seem to contain an answer, place in another csv file corresponding to the specified task. 

e. Human-in-the-loop Labeling: After a user-specified number of examples have been processed by the gatekeepers, present the user with the identified spans of text for confirmation or denial. This can be done in an interactive terminal session, where the user can confirm if the span of text contains the right information or not. This process can be repeated until the user is satisfied with the quality of the labels.

4) Use a user specified model (mine: https://huggingface.co/alvaroalon2/biobert_chemical_ner) or regular expressions to extract the specified answers from each task specified dataset the gatekeepers generated. Make the method used to pull the properties specifiable by the user per task.

4.5) Human-in-the-loop Extraction: After the extraction step, present the user with the extracted answers for confirmation or correction. This can be done in an interactive terminal session, where the user can confirm if the extracted answer is correct or not. If the answer is not correct, the user can provide the correct answer. This process can be repeated until the user is satisfied with the quality of the extracted answers.

5) If more than one chemical name is found per paper, put that paper aside. Accumulate a large number of papers that only have one chemical mention and one set of properties associated with it. Use this data to train a relation extraction model. Then use this model to do relation extraction on the multi-chemical papers. (similar to 'Winograd Schema Challenge') 

6) Once the chemicals have been lined up with their properties, commit them to a csv file along with the doi of the paper they were extracted from.

6.5) Go back and try to pull any information possible from the tables.

7) Using this csv file merged with the 'does not contain' answers from earlier, train/fine-tune a new model to be able to go straight from raw text to chemicals and their properties.

8) Evaluation and Iteration: Identify areas for improvement, and iterate on the approach. This could involve improving the labeling functions, fine-tuning the models, or collecting more data.

9) Error Analysis: After each iteration, perform an error analysis. This involves looking at the examples that your models get wrong and trying to understand why they're making mistakes. This can help you identify areas for improvement and refine your approach.




Rough code outline in terms of functions:

scrape_papers(): This function will scrape papers using your existing mechanisms.

	Done!

convert_to_plaintext(): This function will convert the scraped documents into plaintext using the unstructured package. It will also handle the conversion of tables to CSV format and the routing of these files to a separate folder.

	Yo:
	def doc_to_txt(text_dir):
        # Directory where the PDFs are stored
        pdf_files_dir = str(os.getcwd()) + '/scraped_docs/' + text_dir

        try:
            os.mkdir(str(os.getcwd()) + '/txts/')
        except:
            None

        try:
            os.mkdir(str(os.getcwd()) + '/txts/' + text_dir)
        except:
            None

        # Directory where the text files will be stored
        text_files_dir = str(os.getcwd()) + '/txts/' + text_dir

        # Get the list of PDF files and TXT files
        pdf_files = sorted([filename for filename in os.listdir(pdf_files_dir) if filename.endswith('.pdf')])
        non_pdf_files = sorted([filename for filename in os.listdir(pdf_files_dir) if not filename.endswith('.pdf')])
        txt_files = sorted([filename for filename in os.listdir(text_files_dir) if filename.endswith('.txt')])

        # If there are already some TXT files processed
        if txt_files:
            last_processed_file = txt_files[-1].replace('.txt', '.pdf')
            last_index = pdf_files.index(last_processed_file)
            pdf_files = pdf_files[last_index + 1:]  # Ignore already processed files

        # Convert each PDF to a text file
        for filename in pdf_files:
            pdf_file_path = os.path.join(pdf_files_dir, filename)
            text_file_path = os.path.join(text_files_dir, filename.replace('.pdf', '.txt'))
            print(f"Now working on {filename}")

            try:
                # Partition the PDF into elements
                elements = partition_pdf(filename=pdf_file_path, strategy='hi_res', infer_table_structure=True)

                # Check if elements are empty
                if not elements or all(not str(element).strip() for element in elements):
                    print(f"Skipping {filename} as it does not contain any text.")
                    continue

                # Write the elements to a text file
                with open(text_file_path, 'w') as file:
                    for element in elements:
                        file.write(str(element) + '\n')
            except (PDFSyntaxError, TypeError) as er:
                print(f"Failed to process {filename} due to '{er}'.")
                continue

        for filename in non_pdf_files:
            non_pdf_file_path = os.path.join(pdf_files_dir, filename)
            text_file_path = os.path.join(text_files_dir, filename.replace('.pdf', '.txt'))
            print(f"Now working on {filename}")

            try:
                # Partition the PDF into elements
                elements = partition(filename=non_pdf_file_path)

                # Check if elements are empty
                if not elements or all(not str(element).strip() for element in elements):
                    print(f"Skipping {filename} as it does not contain any text.")
                    continue

                # Write the elements to a text file
                with open(text_file_path, 'w') as file:
                    for element in elements:
                        file.write(str(element) + '\n')
            except Exception as eek:
                print(f"Error processing {filename}; {eek}")
                continue

automated_labeling(): This function will use Snorkel and labeling functions to label spans of text in the data. It will generate labeling functions based on user examples and use these to identify spans of text that are likely to contain an answer.

	Boom:
	def automated_labeling(df_train, tasks):
    from snorkel.labeling import PandasLFApplier
    from snorkel.labeling.model import LabelModel
    from snorkel.types import DataPoint
    from functools import partial

    # Definitions
    def lf_keyword_search(x: DataPoint, keyword) -> int:
        return 1 if keyword in x.text.lower() else 0

    def lf_question_answering(x: DataPoint, nlp, question) -> int:
        answer = nlp(question=question, context=x.text)
        if answer['score'] > 0.5:
            return 1 if answer['answer'].lower() == 'yes' else -1
        else:
            return 0

    def lf_sentence_similarity(x: DataPoint, model, sentence_embedding) -> int:
        x_embedding = model.encode([x.text])
        similarity = cosine_similarity([sentence_embedding], [x_embedding])[0][0]
        if similarity > 0.8:
            return 1
        elif similarity > 0.3:
            return 0
        else:
            return -1

    def lf_sentence_matching(x: DataPoint, sentence) -> int:
        return 1 if re.search(sentence, x.text) else 0

    # Instantiate lfs list
    lfs = []

    # Labeling Functions
    for task in tasks:
        for keyword in task['keywords']:
            lf = partial(lf_keyword_search, keyword=keyword)
            lfs.append(lf)

        for model_id in task['model_identifiers']:
            for question in task['questions']:
                nlpr = pipeline('question-answering', model=model_id)
                lf = partial(lf_question_answering, nlp=nlpr, question=question)
                lfs.append(lf)

        comprehensive_sentences = []
        for sentence in task['sentences']:
            escaped_sentence = escape_nouns(sentence)
            synonym_sentences = generate_synonym_sentences(escaped_sentence)
            comprehensive_sentences.extend(synonym_sentences)

        comprehensive_sentences = list(set(comprehensive_sentences))  # remove duplicates

        for model_id in task['model_identifiers']:
            model = SentenceTransformer(model_id)

            for sentence in comprehensive_sentences:
                sentence_embedding = model.encode([sentence])
                lf = partial(lf_sentence_similarity, model=model, sentence_embedding=sentence_embedding)
                lfs.append(lf)

                lf = partial(lf_sentence_matching, sentence=sentence)
                lfs.append(lf)

    # Apply the labeling functions to your dataset
    applier = PandasLFApplier(lfs)
    L_train = applier.apply(df_train)

    # Train a Snorkel LabelModel to combine the labels
    label_model = LabelModel(cardinality=2, verbose=True)
    label_model.fit(L_train, n_epochs=500, log_freq=100, seed=123)

    # Transform the labels into a single set of noise-aware probabilistic labels
    for task in tasks:
        df_train[task['name'] + "_label"] = label_model.predict(L=L_train, tie_break_policy="abstain")

    return df_train


train_label_model(): This function will train a label model for each task using the generated labeling heuristics and generate a label dataset with it.

	Heh:
	from snorkel.labeling import PandasLFApplier
from snorkel.labeling.model import LabelModel
from snorkel.types import DataPoint
from functools import partial
import pandas as pd

def train_label_model(df_train, tasks):
    """
    This function trains a label model for each task using the generated labeling heuristics and generates a label dataset with it.
    """
    # Define the labeling functions outside the loop
    def lf_keyword_search(x: DataPoint, keyword) -> int:
        return 1 if keyword in x.text.lower() else 0

    def lf_question_answering(x: DataPoint, nlp, question) -> int:
        answer = nlp(question=question, context=x.text)
        if answer['score'] > 0.5:
            return 1 if answer['answer'].lower() == 'yes' else -1
        else:
            return 0

    def lf_sentence_similarity(x: DataPoint, model, sentence_embedding) -> int:
        x_embedding = model.encode([x.text])
        similarity = cosine_similarity([sentence_embedding], [x_embedding])[0][0]
        if similarity > 0.8:
            return 1
        elif similarity > 0.3:
            return 0
        else:
            return -1

    def lf_sentence_matching(x: DataPoint, sentence) -> int:
        return 1 if re.search(sentence, x.text) else 0

    # Instantiate lfs list
    lfs = []

    # Labeling Functions
    for task in tasks:
        for keyword in task['keywords']:
            lf = partial(lf_keyword_search, keyword=keyword)
            lfs.append(lf)

        for model_id in task['model_identifiers']:
            for question in task['questions']:
                nlpr = pipeline('question-answering', model=model_id)
                lf = partial(lf_question_answering, nlp=nlpr, question=question)
                lfs.append(lf)

        comprehensive_sentences = []
        for sentence in task['sentences']:
            escaped_sentence = escape_nouns(sentence)
            synonym_sentences = generate_synonym_sentences(escaped_sentence)
            comprehensive_sentences.extend(synonym_sentences)

        comprehensive_sentences = list(set(comprehensive_sentences))  # remove duplicates

        for model_id in task['model_identifiers']:
            model = SentenceTransformer(model_id)

            for sentence in comprehensive_sentences:
                sentence_embedding = model.encode([sentence])
                lf = partial(lf_sentence_similarity, model=model, sentence_embedding=sentence_embedding)
                lfs.append(lf)

                lf = partial(lf_sentence_matching, sentence=sentence)
                lfs.append(lf)

        # Apply the labeling functions to your dataset
        applier = PandasLFApplier(lfs)
        L_train = applier.apply(df_train)

        # Train a Snorkel LabelModel to combine the labels
        label_model = LabelModel(cardinality=2, verbose=True)
        label_model.fit(L_train, n_epochs=500, log_freq=100, seed=123)

        # Transform the labels into a single set of noise-aware probabilistic labels
        df_train[task['name'] + "_label"] = label_model.predict(L=L_train, tie_break_policy="abstain")

    return df_train
	
	
train_gatekeeper(): This function will train another model (the gatekeeper) for each task to predict based only on the text itself if a span of text contains an answer or not.

	Code:
	def train_gatekeeper(label_model, train_dataset, model_type, model_path):
    """
    Train a gatekeeper model for each task using the label dataset.

    Args:
        label_model (snorkel.labeling.model.LabelModel): The trained label model.
        train_dataset (torch.utils.data.Dataset): The training dataset.
        model_type (str): The type of the Hugging Face model to use.
        model_path (str): The path to save the trained model.

    Returns:
        None
    """
    # Define the training arguments
    training_args = TrainingArguments(
        output_dir='./results',
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
    )

    # Initialize the trainer
    model = AutoModelForQuestionAnswering.from_pretrained(model_type)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=None,  # We don't have a validation dataset here
    )

    # Train the model
    trainer.train()

    # Save the model
    trainer.save_model(model_path)

    # Save the tokenizer and the model's configuration
    tokenizer = AutoTokenizer.from_pretrained(model_type)
    tokenizer.save_pretrained(model_path)
    model.config.save_pretrained(model_path)


run_gatekeeper(): This function will run the gatekeepers on paragraphs from papers one by one, adding any that seem not to contain an answer to a new training dataset.

	Code:
	def run_gatekeeper(gatekeeper_models, texts, user_specified_number):
    """
    Run the gatekeeper models on paragraphs from papers one by one.

    Args:
    gatekeeper_models (dict): A dictionary of gatekeeper models for each task.
    texts (list): A list of texts (paragraphs) from papers.
    user_specified_number (int): The number of examples to process before switching to human-in-the-loop labeling.

    Returns:
    list: A list of tuples, each containing a span of text and an empty string or the predicted answer.
    """
    # Initialize the result list
    results = []

    # Initialize the count of processed examples
    count = 0

    # Process each text
    for text in texts:
        # Process each task
        for task, model in gatekeeper_models.items():
            # Predict if the text contains an answer for the task
            prediction = model.predict([text])

            # If the text seems not to contain an answer, add it to the result list with an empty string as the answer
            if prediction == 0:
                results.append((text, ""))
            else:
                # If the text seems to contain an answer, add it to the result list with the predicted answer
                # Here, you might want to use a different model or method to extract the answer
                answer = extract_answer(model, text)
                results.append((text, answer))

            # Increment the count of processed examples
            count += 1

            # If the count reaches the user-specified number, return the results
            if count >= user_specified_number:
                return results

    # Return the results
    return results


human_in_the_loop_labeling(): This function will present the user with the identified spans of text for confirmation or denial after a user-specified number of examples have been processed by the gatekeepers.

	Code:
	def curate_dataset(df):
    # Iterate over the rows of the DataFrame
    for i, row in df.iterrows():
        # Present the user with the row
        print(row)

        # Ask the user to confirm or deny the row
        while True:
            user_input = input("Is this row correct? (yes/no): ")
            if user_input.lower() in ['yes', 'no']:
                break
            print("Invalid input. Please enter 'yes' or 'no'.")

        # If the row is not correct, ask the user which column to replace and what to replace it with
        if user_input.lower() == 'no':
            while True:
                column = input("Which column number would you like to replace? ")
                if column.isdigit() and int(column) < len(df.columns):
                    break
                print("Invalid input. Please enter a valid column number.")
            replacement = input("What is the replacement string? ")
            df.iat[i, int(column)] = replacement

        # Ask the user if they want to continue
        while True:
            user_input = input("Do you want to continue? (yes/no): ")
            if user_input.lower() in ['yes', 'no']:
                break
            print("Invalid input. Please enter 'yes' or 'no'.")

        # If the user doesn't want to continue, break the loop
        if user_input.lower() == 'no':
            break

    return df



extract_answers(): This function will use a user-specified model or regular expressions to extract the specified answers from each task-specified dataset that the gatekeepers generated.

	Code:
	def extract_answers(task_datasets, model_identifier):
    """
    Function to extract the specified answers from each task-specified dataset
    that the gatekeepers generated.

    Parameters:
    task_datasets (list): List of task-specific datasets generated by the gatekeepers
    model_identifier (str): Identifier for the Hugging Face model to be used for extraction

    Returns:
    extracted_answers (list): List of extracted answers
    """
    # Initialize the Hugging Face model
    model = pipeline('question-answering', model=model_identifier)

    extracted_answers = []

    # Loop over each task-specific dataset
    for task_dataset in task_datasets:
        task_answers = []

        # Loop over each example in the task-specific dataset
        for example in task_dataset:
            # Extract the answer from the example using the model
            answer = model(question=example['question'], context=example['context'])
            task_answers.append(answer)

        extracted_answers.append(task_answers)

    return extracted_answers


human_in_the_loop_extraction(): This function will present the user with the extracted answers for confirmation or correction after the extraction step.

	Code:
	def human_in_the_loop_extraction(task, model, tokenizer):
    # Load the DataFrame from the CSV file
    df_train = pd.read_csv('snorkel/training_data.csv')

    # Filter the DataFrame to only include rows where the gatekeeper predicted an answer
    df_train = df_train[df_train[task + "_label"] == 1]

    # Initialize the question-answering pipeline
    nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)

    # Iterate over the rows in the DataFrame
    for i, row in df_train.iterrows():
        # Extract the answer using the question-answering pipeline
        answer = nlp(question=task, context=row['text'])

        # Present the extracted answer to the user for confirmation or correction
        print(f"Extracted answer for task '{task}' in text '{row['text']}': {answer['answer']}")
        user_input = input("Is this answer correct? (yes/no): ")

        # If the user says the answer is not correct, allow them to provide the correct answer
        if user_input.lower() != 'yes':
            correct_answer = input("Please provide the correct answer: ")
            df_train.loc[i, 'extracted_answer'] = correct_answer
        else:
            df_train.loc[i, 'extracted_answer'] = answer['answer']

    # Save the DataFrame to a CSV file
    df_train.to_csv('snorkel/training_data.csv', index=False)


train_relation_extraction_model(): This function will train a relation extraction model using papers that only have one chemical mention and one set of properties associated with it.

	Code:
	def train_relation_extraction_model(df_train, task_name, model_type, model_path):
    """
    This function trains a relation extraction model using papers that only have one chemical mention and one set of properties associated with it.

    Args:
        df_train (pd.DataFrame): The DataFrame containing the training data.
        task_name (str): The name of the task.
        model_type (str): The Hugging Face model type for the final model.
        model_path (str): The path to save the trained model.

    Returns:
        None
    """
    # Filter the DataFrame to only include papers with one chemical mention
    df_train_single_chemical = df_train[df_train[task_name + "_label"] == 1]

    # Prepare the dataset for training
    tokenizer = AutoTokenizer.from_pretrained(model_type)
    train_encodings = tokenizer(df_train_single_chemical['text'].tolist(), truncation=True, padding=True)

    # Create a dataset and split into training and validation sets
    labels = df_train_single_chemical[task_name + "_label"].tolist()
    dataset = SnorkelDataset(train_encodings, labels)

    # Split the dataset for this task into training and validation sets
    train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)

    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)

    # Define the training arguments
    training_args = TrainingArguments(
        output_dir='./results',
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
    )

    # Initialize the trainer
    model = AutoModelForQuestionAnswering.from_pretrained(model_type)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    # Train the model
    trainer.train()

    # Save the model
    trainer.save_model(model_path)

    # Save the tokenizer and the model's configuration
    tokenizer.save_pretrained(model_path)
    model.config.save_pretrained(model_path)


extract_relations(): This function will use the trained relation extraction model to do relation extraction on the multi-chemical papers.

	Code:
	def extract_relations(task, model_path, text_dir):
    """
    This function uses a trained relation extraction model to extract relations from the multi-chemical papers.
    
    Args:
        task (dict): The task for which the relations are to be extracted.
        model_path (str): The path to the trained relation extraction model.
        text_dir (str): The directory where the text files are stored.
    """
    
    # Load the trained relation extraction model
    model = AutoModelForQuestionAnswering.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # Load the text files
    texts = load_text_files(text_dir)
    
    # Convert texts to DataFrame
    df = pd.DataFrame(texts, columns=['text'])
    
    # Initialize the pipeline for question-answering
    nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)
    
    # Initialize an empty DataFrame to store the results
    df_results = pd.DataFrame(columns=['chemical', 'property', 'value'])
    
    # For each text, extract the relations
    for text in df['text']:
        for question in task['questions']:
            answer = nlp(question=question, context=text)
            if answer['score'] > 0.5:  # If the model is confident in its answer
                # Extract the chemical, property, and value from the answer
                chemical, property, value = extract_chemical_property_value(answer['answer'])
                
                # Append the result to the DataFrame
                df_results = df_results.append({'chemical': chemical, 'property': property, 'value': value}, ignore_index=True)
    
    # Return the DataFrame with the results
    return df_results


commit_to_csv(): This function will commit the chemicals lined up with their properties to a CSV file along with the DOI of the paper they were extracted from.

	Code:
	def commit_to_csv(df, task_names, output_dir):
    """
    This function commits the chemicals lined up with their properties to a CSV file 
    along with the DOI of the paper they were extracted from.

    Parameters:
    df (pandas.DataFrame): The DataFrame containing the text and labels.
    task_names (list): The list of task names.
    output_dir (str): The directory where the CSV file will be saved.
    """
    # Create a new DataFrame to store the results
    results_df = pd.DataFrame()

    # For each task, extract the chemicals and their properties
    for task_name in task_names:
        # Get the labels for this task
        labels = df[task_name + "_label"]

        # Get the text that contains an answer for this task
        results_df[task_name] = df['text'][labels == 1]

    # Add the DOI of the paper
    # Assuming that the DOI is stored in a column named 'doi' in the original DataFrame
    results_df['doi'] = df['doi']

    # Save the DataFrame to a CSV file
    results_df.to_csv(os.path.join(output_dir, 'results.csv'), index=False)


extract_table_info(): This function will go back and try to pull any information possible from the tables.

	Code:
	def extract_table_info(directory):
    """
    This function extracts information from tables in the documents.
    """
    # Directory where the CSV files (converted tables) are stored
    csv_files_dir = str(os.getcwd()) + '/tables/' + directory

    # Get the list of CSV files
    csv_files = sorted([filename for filename in os.listdir(csv_files_dir) if filename.endswith('.csv')])

    # Initialize a DataFrame to store the extracted information
    df_table_info = pd.DataFrame()

    # Process each CSV file
    for filename in csv_files:
        csv_file_path = os.path.join(csv_files_dir, filename)
        print(f"Now working on {filename}")

        try:
            # Load the table into a DataFrame
            df_table = pd.read_csv(csv_file_path)

            # Process the table to extract the information (this will depend on your specific needs)
            # For example, you might want to extract certain columns, or rows that meet certain criteria
            extracted_info = process_table(df_table)

            # Append the extracted information to df_table_info
            df_table_info = df_table_info.append(extracted_info, ignore_index=True)

        except Exception as e:
            print(f"Failed to process {filename} due to '{e}'.")
            continue

    # Return the DataFrame containing the extracted information
    return df_table_info


def process_table(df_table):
    """
    This function processes a table to extract the required information.
    The implementation of this function will depend on your specific needs.
    """
    # For example, you might want to extract certain columns, or rows that meet certain criteria
    # Here's a simple example that just returns the entire table
    return df_table
	
pretrain_final_model(): This function will use the original plaintext corpus to pre-train the model

import os
import json
import subprocess
from argparse import ArgumentParser
from pytorch_lightning import Trainer
from rwkv.model import RWKV
from rwkv.src.binidx import MMapIndexedDataset

def pretrain_final_model():

    # Set environment variables for RWKV
    os.environ["RWKV_JIT_ON"] = '1'
    os.environ["RWKV_CUDA_ON"] = '1'  # '1' to use CUDA kernel for seq mode (much faster)

    # Clone the json2binidx_tool repository if it doesn't exist
    if not os.path.exists(os.path.join(os.getcwd(), 'json2binidx_tool')):
        subprocess.run(['git', 'clone', 'https://github.com/Abel2076/json2binidx_tool.git'])

    # Download the tokenizer file if it doesn't exist
    tokenizer_file = os.path.join(os.getcwd(), '20B_tokenizer.json')
    if not os.path.exists(tokenizer_file):
        subprocess.run(['wget', '-O', tokenizer_file, 'https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-v4/20B_tokenizer.json'])

    # Specify the output path for the .jsonl file
    output_path = os.path.join(os.getcwd(), 'compiled_docs', output_directory_id)

    with open(output_path, 'w') as out:
        for filename in os.listdir(text_dir):
            if filename.endswith('.txt'):
                with open(os.path.join(text_dir, filename), 'r') as f:
                    text = f.read()
                    json_obj = json.dumps({"text": text}, ensure_ascii=False)
                    out.write(json_obj + "\n")

    # Specify the path to the preprocess_data.py script
    preprocess_script = os.path.join(os.getcwd(), 'json2binidx_tool', 'preprocess_data.py')

    # Run the preprocess_data.py script
    subprocess.run(['python', preprocess_script, '--input', output_path, '--output-prefix', output_path, '--vocab', tokenizer_file, '--dataset-impl', 'mmap', '--tokenizer-type', 'HFTokenizer', '--append-eod'])

	if auto is None:
		# Specify the path to your model
		model_path = input("Enter the path you want to save your model to:")

    # Load the model
    model = RWKV.load_from_checkpoint(model_path)

    # Create a dataset from your .bin and .idx files
    dataset = MMapIndexedDataset(output_path)

    # Create a trainer
    trainer = Trainer(gpus=1, precision=16, accelerator='ddp')

    # Train the model
    trainer.fit(model, dataset)





train_final_model(): This function will train a new model to be able to go straight from raw text to chemicals and their properties using the CSV file merged with the 'does not contain' answers from earlier.

	Code:
	def train_final_model(df_train, model_type, best_hyperparameters, model_path):
    """
    Train a new model to be able to go straight from raw text to chemicals and their properties.
    """

    # Define a PyTorch dataset
    class SnorkelDataset(torch.utils.data.Dataset):
        def __init__(self, encodings, labels):
            self.encodings = encodings
            self.labels = labels

        def __getitem__(self, idx):
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            item['labels'] = torch.tensor(self.labels[idx])
            return item

        def __len__(self):
            return len(self.encodings.input_ids)

    # Prepare the dataset for training
    tokenizer = AutoTokenizer.from_pretrained(model_type)
    train_encodings = tokenizer(df_train['text'].tolist(), truncation=True, padding=True)

    # Create a dataset and split into training and validation sets
    labels = df_train["final_label"].tolist()
    dataset = SnorkelDataset(train_encodings, labels)

    # Split the dataset into training and validation sets
    train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)

    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)

    # Define the training arguments
    training_args = TrainingArguments(
        output_dir='./results',
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
    )

    # Initialize the trainer
    model = AutoModelForQuestionAnswering.from_pretrained(model_type)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    # Train the model with the best hyperparameters
    training_args.learning_rate = best_hyperparameters["learning_rate"]
    training_args.num_train_epochs = best_hyperparameters["num_train_epochs"]
    training_args.per_device_train_batch_size = best_hyperparameters["per_device_train_batch_size"]
    training_args.warmup_steps = best_hyperparameters["warmup_steps"]

    trainer.train()

    # Save the model
    trainer.save_model(model_path)

    # Save the tokenizer and the model's configuration
    tokenizer.save_pretrained(model_path)
    model.config.save_pretrained(model_path)


evaluate_and_iterate(): This function will identify areas for improvement, and iterate on the approach. This could involve improving the labeling functions, fine-tuning the models, or collecting more data.

	Code:
	def evaluate_and_iterate(trainer, model_path, train_dataset, val_dataset, study, objective, n_trials=50):
    """
    This function identifies areas for improvement, and iterates on the approach.
    This could involve improving the labeling functions, fine-tuning the models, or collecting more data.
    """

    # Evaluate the model
    eval_result = trainer.evaluate()

    # Print the evaluation result
    print("Evaluation result: ", eval_result)

    # Optimize the hyperparameters with Optuna
    study.optimize(objective, n_trials=n_trials)

    # Print the best hyperparameters
    print("Best trial:")
    trial = study.best_trial
    print(" Value: ", trial.value)
    print(" Params: ")
    for key, value in trial.params.items():
        print(f"    {key}: {value}")

    # Train the model with the best hyperparameters
    model = AutoModelForQuestionAnswering.from_pretrained(model_type)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )
    trainer.train()

    # Save the model
    trainer.save_model(model_path)

    # Save the tokenizer and the model's configuration
    tokenizer.save_pretrained(model_path)
    model.config.save_pretrained(model_path)


perform_error_analysis(): This function will perform an error analysis after each iteration, looking at the examples that your models get wrong and trying to understand why they're making mistakes.

	Code:
	def perform_error_analysis(model, val_dataset, predictions):
    # Get the actual labels from the validation dataset
    actual_labels = [example['labels'].item() for example in val_dataset]

    # Compare the predicted labels to the actual labels to find the indices of the examples that the model got wrong
    wrong_indices = [i for i, (pred, actual) in enumerate(zip(predictions, actual_labels)) if pred != actual]

    # For each wrong example, print out the text, the predicted label, and the actual label
    for i in wrong_indices:
        example = val_dataset[i]
        text = tokenizer.decode(example['input_ids'])
        pred_label = predictions[i]
        actual_label = actual_labels[i]
        print(f"Text: {text}")
        print(f"Predicted label: {pred_label}")
        print(f"Actual label: {actual_label}")
        print("\n")



	And here's the placeholder for the actual run:
	
	import os
import re
import torch
import pandas as pd
from functools import partial
from sklearn.model_selection import train_test_split
from torch.utils.data import Subset
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer
from snorkel.labeling import PandasLFApplier
from snorkel.labeling.model import LabelModel
from snorkel.types import DataPoint
from unstructured import partition_pdf
from PyPDF2.utils import PDFSyntaxError
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline

# Your functions go here

def main():
    # Define your parameters here
    tasks = []
    df_train = pd.DataFrame()
    model_type = ""
    model_path = ""
    text_dir = ""
    user_specified_number = 0
    best_hyperparameters = {}
    n_trials = 50
    directory = ""
    output_dir = ""
    study = None
    objective = None
    predictions = []

    # Run the pipeline
    scrape_papers()
    doc_to_txt(text_dir)
    df_train = automated_labeling(df_train, tasks)
    df_train = train_label_model(df_train, tasks)
    train_gatekeeper(label_model, train_dataset, model_type, model_path)
    results = run_gatekeeper(gatekeeper_models, texts, user_specified_number)
    df_train = human_in_the_loop_labeling(df_train)
    extracted_answers = extract_answers(task_datasets, model_identifier)
    human_in_the_loop_extraction(task, model, tokenizer)
    train_relation_extraction_model(df_train, task_name, model_type, model_path)
    df_results = extract_relations(task, model_path, text_dir)
    commit_to_csv(df, task_names, output_dir)
    df_table_info = extract_table_info(directory)
    train_final_model(df_train, model_type, best_hyperparameters, model_path)
    evaluate_and_iterate(trainer, model_path, train_dataset, val_dataset, study, objective, n_trials)
    perform_error_analysis(model, val_dataset, predictions)

if __name__ == "__main__":
    main()
